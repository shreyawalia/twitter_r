#load tweets from file and doing
library(rtweet)
library(dplyr)
library(tidyverse)
library(readr)
library(ggplot2)
twitter.data <- read_csv("Twitter_Data.csv")

#Finding the source of the tweets:
tweets <- twitter.data %>%
  select(user_id, source, text, created_at)
device.count<- tweets %>%  
              count(source, sort = TRUE) %>%
              mutate(source= reorder(source, n)) %>%
              top_n(10) %>%
              ggplot(aes(source, n, fill=source)) +
              geom_bar(stat = "identity") +
              ylab(NULL)+
              theme(legend.position="none")+
              coord_flip()
device.count

#Cleaning up tweets for analysis
#Finding what is original and what is retweeted
 #clean up tweet
library(lubridate)
tweets1 <- twitter.data %>% 
    select(text, created_at, retweet_count)
#finding what is retweeted 
retweets <-tweets1 %>%
    filter(retweet_count>0) %>%
    mutate(type="Retweet")

no.retweets <- tweets1 %>%
    filter(retweet_count==0) %>%
    mutate(type="Orig.Tweet") 

tweets2 <- bind_rows(retweets, no.retweets)

ggplot(tweets2, aes(x = created_at, fill=type)) +
    geom_histogram(alpha = 0.5, position = "identity")

rm(no.retweets,retweets)

#Claculating the frequency of words used in tweets using unnest_tokenization
#Cleaning data
library(tidytext)
tweets3 <- twitter.data %>% 
    select(text, created_at, retweet_count)

#what's getting retweeted 
retweets <-tweets3 %>%
    filter(retweet_count>0) %>%
    mutate(type="Retweets")

no.retweets <- tweets3 %>%
    filter(retweet_count==0) %>%
    mutate(type="No.Retweets") 

tweets4 <- bind_rows(retweets, no.retweets)

clean.tweets <- tweets4 %>% 
    select(text,type, created_at)%>%
    mutate(text=iconv(text, "latin1", "ASCII", "")) %>%
    mutate(text=tolower(text))

#Using wordcloud for getting most frequent words
library(wordcloud)
tidy.all <- clean.tweets %>% 
    unnest_tokens(word, text, token = "tweets")%>%
    filter(!word %in% stop_words$word,str_detect(word, "[a-z]"))

frequency.all<- tidy.all %>% 
    count(word, sort = TRUE) 

head(frequency.all,15)

#Using wordcloud for plotting most frequent words
tidy.all%>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))

#calculating logratio for no.retweets and retweets
word_ratios <- tidy.all %>%
  filter(!str_detect(word, "^@")) %>%
  count(word, type) %>%
  filter(sum(n) >= 5) %>%
  spread(type, n, fill = 0) %>%
  ungroup() %>%
  mutate_if(is.numeric, list(~(. + 1) / (sum(.) + 1))) %>%
  #mutate_each(funs((. + 1) / sum(. + 1)), -word) %>%
  mutate(logratio = log(No.Retweets / Retweets)) %>%
  arrange(desc(logratio))


word_ratios %>%
  group_by(logratio < 0) %>%
  top_n(15, abs(logratio)) %>%
  ungroup() %>%
  mutate(word = reorder(word, logratio)) %>%
  ggplot(aes(word, logratio, fill = logratio < 0)) +
  geom_bar(alpha = 0.8, stat = "identity") +
  coord_flip() +
  ylab("log odds ratio (No.Retweets / Retweets)") +
  scale_fill_discrete(name = "", labels = c("No.Retweets", "Retweets"))

#Doing sentiment analysis on the tweets
#Using Bing
bing <- get_sentiments("bing")

bing.sentiment.counts <- frequency.all %>%
                inner_join(bing)

bing.sentiment.counts %>%
  filter(n > 100) %>%
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ylab("Contribution to sentiment")

#Doing sentiment analysis of tweets using nrc
library(textdata)
library(reshape2)
tidy.all %>%
  inner_join(get_sentiments("nrc")) %>%
  count(sentiment, sort=TRUE)%>%
  ggplot(aes(sentiment, n, fill=sentiment)) +
  geom_bar(stat = "identity") +
  theme(legend.position="none")+
  labs(title = "Sentiment for HongKong Election Result")

tidy.all %>%
  inner_join(get_sentiments("nrc")) %>%
  filter(!sentiment %in% c("positive", 
                          "negative"))%>%
  count(word,sentiment, sort=TRUE)%>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = brewer.pal(8, "Dark2"),
                   title.size=1.5, max.words=300)
```



