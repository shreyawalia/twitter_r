---
title: 'Assignment #5 - AMOD 5250H'
author: "Your Name Here"
date: 'release date: 11/11/2019'
output:
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '3'
  html_notebook:
    df_print: paged
    highlight: tango
    self_contained: yes
    theme: paper
    toc: yes
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(tidyverse)


```
**Make sure your solution to each question includes enough output to show that your solution is correct (i.e if asked to create a vector, output the vector afterwards)**


#Question 1 - Two Sample Statistical Tests [9 marks]
Load *salaries.csv*, which contains the average male & female salaries (in 1000s) from 50 random collages in the US, and *BbVsFb.csv*, which contains weights of randomly sampled professional football and basketball players. Note that the two salary columns are dependant data, while the professional player data is independent. For each set of data (note: you are comparing the data within each set, not the sets), complete the following:

a. Test each sample set for normalcy. Summarize the results. [3 marks]
```{r}

#Code here
salary <- read.csv("salaries.csv")
#Checking normality for male salary data
shapiro.test(salary$males)
qqnorm(salary$males)

#Checking normality for female salary data
shapiro.test(salary$females)
qqnorm(salary$females)

players <- read.csv("BbVsFb.csv")
#Checking normality for football players data
shapiro.test(players$football)
qqnorm(players$football)

#Checking normality for basketball players data
shapiro.test(players$basketball)
qqnorm(players$basketball)

# As the data in salary table is dependent we can use the following test to check for dependent data
dependent_data_normality <- with(salary, 
        salary$males - salary$females)
shapiro.test(dependent_data_normality)
```
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

Text Answer in here

From the shaipro-wilk test on all the sets, the p-values are greater than the significance level 0.05 implying that the distribution of the data are not significantly different from the normal distribution. In other words, we can assume the normality of data for all the data sets.
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

b. Test to see if the sample variances are significantly different. Explain the results [3 marks]

```{r}
#Code here
# F-test for independent data
res.ftest <- var.test(players$football, players$basketball, data = players, var.equal = TRUE)
res.ftest

#F-test for dependent data
res.ftest1 <- var.test(salary$males, salary$females, data = salary,  paired = TRUE)
res.ftest1
```
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

Text Answer in here

For players data:
The p-value of F-test is p = 0.765. It's greater than the significance level alpha = 0.05. In conclusion, there is no significant difference between the variances of the two sets of data. 

For salary data:
The p-value of F-test is p = 0.9445. It's greater than the significance level alpha = 0.05. In conclusion, there is no significant difference between the variances of the two sets of data. 
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

c. Statistically compare the means of each set. What are the results? [3 marks]
```{r}
#Code here
## T-test for independent players data
res <- t.test(players$football, players$basketball, var.equal = TRUE)
res
## T-test for dependent salary data
res1 <- t.test(salary$males, salary$females, paired = TRUE)
res1
```
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

Text Answer in here

For players data:
The p-value of the test is 3.2e-16, which is less than the significance level alpha = 0.05. We can conclude that football players average weight is significantly different from basketball players average weight with a p-value = 3.2e-16.

For salary data:
The p-value of the test is 0.2127, which is greater than the significant level alpha = 0.05. We can conclude that there is no significant difference in salaries of males and females.
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@


#Question 2 - Linear Regresssion [10 marks]

Load *Insurance.csv*, which contains auto insurance information for regions Sweden. Where `X = number of claims` and `Y = total payment for all the claims in thousands` (Swedish Kronor).

a. Use summary and plot to investigate the data. Identify anything worth noting. [2 marks]
```{r}
#Code here
insurance <- read.csv("Insurance.csv")
head(insurance)
summary(insurance)
ggplot(insurance, aes(x=x, y=y)) +
    geom_point(color="purple") +
    ylab("number of claims")+
    xlab("total payment for all the claims in thousands")+
    ggtitle("Relationship between claims and total payment for all the claims")+
    coord_flip()


```
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

Text Answer in here
We can observe that there is a linear relationship between the number of claims and total payment.
We also observe that there are some outliers in the table that have very high number of claims and very high toatl payments.

@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

b. Perform a simple linear regression to generate a model for the relationship. [1 mark] 
```{r}
#Code here
model <- lm(formula = y ~ x, data = insurance)

ggplot(insurance, aes(x=x, y=y))+
    geom_point() +    
    geom_smooth(method=lm)
```

c. Plot the model for evaulation and summarize the results. [3 marks]

```{r}
#Code here
par(mfrow=c(2,2))
plot(model)
```
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

Text Answer in here
1.Residuals vs. Fitted:
The plot shows error residuals vs fitted values here dotted line at y=0 indicates our fit line.
In our graph we can see that the red line that provides an idea of residual movement is approximately a horizontal line and the spread appears to be unbiased but there is some heteroscedasity.

2.Normal QQ plot:
It is used to check if our residuals follow a normal distribution.The residuals are normally distributed if the points follow the dotted line closely.
In our graph we can say that the points follow a normal distribution as they lie close to dotted line.

3.Scale-location plot:
It indicates the spread of points across predicted values range.A horizontal red line indicate that residuals have uniform variance across the range.
In our graph we can say that the line goes slightly up near the middle where the residual points are spread apart.

4.Residuals vs Leverage:
It helps to meausre the influence that a point will have on the predicted score. Here the observation that lie outside the cook's line have potential for influencing our model is higher if we exclude that point. 
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

d. Output the summary of your model and explain the relevent things it tells you. Use inline r-markdown where relevant. [3 marks]

```{r}
#Code here
summary(model)

```
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

Text Answer in here

Explaination of the results of the summary:

1. Call:
The Formaula Call in the output is the formula R used to fit the data. Here we are taking y(total payment for all the claims in thousands) as the target variable  using X(number of claims) as predictor variable i.e. we can calculate the total payment for all the claims by number of claims.

2.Residuals:
Residuals are essentially the difference between the actual observed response values and the response values that the model predicted based on the best fit line. It helps to test the quality of the fit of the model.
In this model there is a symmetric distribution across the points.

3.Coefficients:
The coefficients are two unknown constants that represent the intercept and slope terms in the linear model. Here the intercept("a") and slope("b") values can be found for a relation y = a + b*x.

->Coefficent Estimate:
It is the value of the intercept and slope calculated by the regression. In this model the value of intercept is 19.9945 and value of slope is 3.413. Let us assume that we have number of accidents as 75, we can calculate total claims by using regression as :
y = 19.9945 + 3.413 * 75, y = 274.99
In other words the average no of claims is 19.9945 and for every one claim the value increases by 3.413.

->Coefficent Standard Error:
The coefficient standard error measures the average amount that the coefficient estimates vary from the actual average value of our response variable. Here the standard error for intercept is 6.3678 and for slope is 0.1955. In other words the increase in total claim payment value for 1 claim can vary by 0.1955.

->Coefficent t-value:
The coefficient t-value is a measure of how many standard deviations our coefficient estimate is away from 0.In our model the t-value of intercept(3.14) and slope(17.46) is far from 0 so we can reject null hypothesis.

->Coefficent Pr(>t)
The Pr(>t) acronym found in the model output relates to the probability of observing any value equal or larger than t. If the probability is < 0.05 we reject null hypothesis. In the model the p value is < 0.05 so we reject null hypothesis and accept that there is a relation between no of claims and total payment of claims.

4.Residual Standard Error:
It is measure of the quality of a linear regression fit and is the average amount that the response (total payment of claims) will deviate from the true regression line. In this model it is 35.94. It means that the mean no of claims i.e. 19.994 and residual standard error is 35.94 , the percentage error will be off by .

5.Multiple R-squared:
The models that fit the data well, R² is near 1. Models that poorly fit the data have R² near 0. In the model, the first one has an R² of 0.833 this means that the model explains 83.3% of the total variability.

6.Adjusted R-squared:
Adjusted R² is used as Multiple R-squared cannot decrease as we add more independent variables to model. If more than one variable is being added to the model adjusted R-squared is used, since it only increases if it reduces the overall error of the predictions.
Here adjusted R-square is  0.8306.

7.F-statistic:
F-statistic is an indicator of whether there is a relationship between the predictor and the response variables. Here the value is 305 which is relatively larger than 1 and also p < .05 so it is concluded that there is a relationship between number of claims and total payment for all the claims in thousands.



@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

e. Use the model to predict the total claim, if the number of accidents is 80 and 150. [1 marks]

```{r}
#Code here
a <- data.frame(x = c(80, 150))
result <-  predict(model,a)
print(result)
```


#Question 3 - Twitter [20 marks]

**Note: because of the transiant nature of Twitter, you'll never be able to pull the same data twice.  Please do your data collection in an R Script, and save the resulting dataframe to a file.  Include your code in the un-executing block below so I can see it...but the Markdown file should load your collected data from the file(s) you created. **

Go to Twitters search page (https://twitter.com/search-home?lang=en) and find a hashtag that is currently trending. 

In a seperate R Script, use the rTweet library to access Twitter's REST API and pull any historical tweets with that hashtag.  

Then use Twitters Streaming API to collect real-time tweets with the same hashtag for several hours.  

Parse both collections into dataframes and add a variable indicating which method it was collected with.  Then combine the data frames into one.  Export this to a file, and include your code in the un-executing block below.

Import the data from the file you created and use this data frame and the tidy text library to do some relevent text analaysis (this should include cleaning, tokenizing, frequencies, and a couple of interesting graphs)

```{r eval=FALSE}
#The code used used to collect your tweets should go here
library(rtweet)
library(dplyr)
library(tidyverse)
library(readr)
rtweets <- search_tweets("#HongKongElections", n = 18000, include_rts = FALSE)
rtweets <- as.data.frame(rtweets)
head(rtweets)
save_as_csv(rtweets, "Rest.Tweets.csv", prepend_ids = TRUE, na = "",
            fileEncoding = "UTF-8")
Rest_Tweets <- read_csv("Rest.Tweets.csv")

stream_tweets("#HongKongElections",timeout = 60 * 60 * 7,
              file_name = "stweets.json",
             parse = FALSE
)

Stream.Tweets <- parse_stream("stweets.json")

save_as_csv(Stream.Tweets, "Stream.Tweets.csv", prepend_ids = TRUE, na = "",
          fileEncoding = "UTF-8")
Stream_Tweets <- read_csv("Stream.Tweets.csv")

Twitter_Data <- rbind(Rest_Tweets, Stream_Tweets)
head(Twitter_Data)
save_as_csv(Twitter_Data, "Twitter_Data.csv", prepend_ids = TRUE, na = "",
            fileEncoding = "UTF-8")
```

```{r}
#load tweets from file and do analysis here
library(rtweet)
library(dplyr)
library(tidyverse)
library(readr)
library(ggplot2)
twitter.data <- read_csv("Twitter_Data.csv")

#Finding the source of the tweets:
tweets <- twitter.data %>%
  select(user_id, source, text, created_at)
device.count<- tweets %>%  
              count(source, sort = TRUE) %>%
              mutate(source= reorder(source, n)) %>%
              top_n(10) %>%
              ggplot(aes(source, n, fill=source)) +
              geom_bar(stat = "identity") +
              ylab(NULL)+
              theme(legend.position="none")+
              coord_flip()
device.count

#Cleaning up tweets for analysis
#Finding what is original and what is retweeted
 #clean up tweet
library(lubridate)
tweets1 <- twitter.data %>% 
    select(text, created_at, retweet_count)
#finding what is retweeted 
retweets <-tweets1 %>%
    filter(retweet_count>0) %>%
    mutate(type="Retweet")

no.retweets <- tweets1 %>%
    filter(retweet_count==0) %>%
    mutate(type="Orig.Tweet") 

tweets2 <- bind_rows(retweets, no.retweets)

ggplot(tweets2, aes(x = created_at, fill=type)) +
    geom_histogram(alpha = 0.5, position = "identity")

rm(no.retweets,retweets)

#Claculating the frequency of words used in tweets using unnest_tokenization
#Cleaning data
library(tidytext)
tweets3 <- twitter.data %>% 
    select(text, created_at, retweet_count)

#what's getting retweeted 
retweets <-tweets3 %>%
    filter(retweet_count>0) %>%
    mutate(type="Retweets")

no.retweets <- tweets3 %>%
    filter(retweet_count==0) %>%
    mutate(type="No.Retweets") 

tweets4 <- bind_rows(retweets, no.retweets)

clean.tweets <- tweets4 %>% 
    select(text,type, created_at)%>%
    mutate(text=iconv(text, "latin1", "ASCII", "")) %>%
    mutate(text=tolower(text))

#Using wordcloud for getting most frequent words
library(wordcloud)
tidy.all <- clean.tweets %>% 
    unnest_tokens(word, text, token = "tweets")%>%
    filter(!word %in% stop_words$word,str_detect(word, "[a-z]"))

frequency.all<- tidy.all %>% 
    count(word, sort = TRUE) 

head(frequency.all,15)

#Using wordcloud for plotting most frequent words
tidy.all%>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))

#calculating logratio for no.retweets and retweets
word_ratios <- tidy.all %>%
  filter(!str_detect(word, "^@")) %>%
  count(word, type) %>%
  filter(sum(n) >= 5) %>%
  spread(type, n, fill = 0) %>%
  ungroup() %>%
  mutate_if(is.numeric, list(~(. + 1) / (sum(.) + 1))) %>%
  #mutate_each(funs((. + 1) / sum(. + 1)), -word) %>%
  mutate(logratio = log(No.Retweets / Retweets)) %>%
  arrange(desc(logratio))


word_ratios %>%
  group_by(logratio < 0) %>%
  top_n(15, abs(logratio)) %>%
  ungroup() %>%
  mutate(word = reorder(word, logratio)) %>%
  ggplot(aes(word, logratio, fill = logratio < 0)) +
  geom_bar(alpha = 0.8, stat = "identity") +
  coord_flip() +
  ylab("log odds ratio (No.Retweets / Retweets)") +
  scale_fill_discrete(name = "", labels = c("No.Retweets", "Retweets"))

#Doing sentiment analysis on the tweets
#Using Bing
bing <- get_sentiments("bing")

bing.sentiment.counts <- frequency.all %>%
                inner_join(bing)

bing.sentiment.counts %>%
  filter(n > 100) %>%
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ylab("Contribution to sentiment")

#Doing sentiment analysis of tweets using nrc
library(textdata)
library(reshape2)
tidy.all %>%
  inner_join(get_sentiments("nrc")) %>%
  count(sentiment, sort=TRUE)%>%
  ggplot(aes(sentiment, n, fill=sentiment)) +
  geom_bar(stat = "identity") +
  theme(legend.position="none")+
  labs(title = "Sentiment for HongKong Election Result")

tidy.all %>%
  inner_join(get_sentiments("nrc")) %>%
  filter(!sentiment %in% c("positive", 
                          "negative"))%>%
  count(word,sentiment, sort=TRUE)%>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = brewer.pal(8, "Dark2"),
                   title.size=1.5, max.words=300)
```



